---
title: 4. Comparing two means with the t-test
tags: TeXt
article_header:
  type: cover
  image:
---

The binomial test I introduced in the previous chapter has a great didactic advantage (which is why I introduced it first): it allows to calculate *p* values directly from the observed numbers. Tests that work like this are called *exact tests*. Unfortunately, not every test scenario makes it that easy for us. To be honest, most test scenarios don't. While plain count data is comparatively easy to treat with probabilistic concepts, many measurements we encounter in the real world of research produce more complex sets of numbers (often even with decimals and the like). To construct a null hypothesis and estimate probabilities for such measurements, an additional layer of abstraction is needed; instead of the *exact* numbers themselves, analysis has to be made on derived statistics. At this point, we have to familiarize with another important concept, the *statistical distributions*

## Statistical distributions

**How to model data created by random chance?**

A meaningful null hypothesis assumes that observed data has been created by random processes. A statistical test provides is with an estimate for the probability of our observation under these random conditions. Accordingly, a statistical test is based on model of random data generation for a specific scenario. Mathematicians have described a large number of probabilistic processes that produce random numbers in different types and shapes. These are called *statistical distributions*. 

The simplest statistical distribution is the **uniform distribution**. Here, the spectrum of potential observed values has a lower and upper limit, and all values are equally likely. For an example histogram of a randomly generated uniform distribution see Fig.1.

![Figure 1: The uniform distribution](uniform.png)

*Figure 1: The uniform distribution*

Now, let us assume a more complex random process. Imagine an archaeological excavation. The entire area under study is divided in equally sized squares, and our task is to count the occurrences of a specific type of archaeological object, let us say arrow heads, in each of the squares. In this case, a random process resulting in a uniform distribution seems not very plausible. The type of model we are looking for has two important features:

1. We have have a count of occurrences as a natural number for each discrete unit.

2. There is no natural upper limit to the number of occurrences in in each unit. The lower limit however is 0!

This type of probabilistic process can be modeled with the **Poisson distribution** (Fig. 2). This is not a weird reference to some attribute of aquatic vertebrates, it was just invented by a mathematician named SimÃ©on Denis Poisson.

![Figure 2: The Poisson distribution](/assets/images/poisson.png)

*Figure 2: The Poisson distribution*

According to his model, the probability to find $k$ arrow heads in a square is:

$$
P_\lambda (k) = \frac{\lambda^k}{k!}\, \mathrm{e}^{-\lambda}
$$

Here, $\lambda$ is the parameter describing the center of the distribution: its most probable value.

In many cases, observed values do have an upper limit. For instance, we often measure proportion or percentages in research that have an upper limit at 100%. These cases can be modeled with the **binomial distribution** (Fig. 3), we already did that, though unconsciously, for the binomial test in Chapter 3. 

![Figure 3: The binomial distribution](/assets/images/binomial.png)

*Figure 3: The binomial distribution*

The mathematical expression for the probability that a sample of $n$ observations contains $k$ *successes* (or *heads*, or other binary events), given a single trial success probability of $p$ is:

$$
P_k = \binom nk p^k (1-p)^{n-k}
$$

Again, this model has one parameter: the single trial success probability $p$.

The most famous among the statistical distributions is the **normal distribution** AKA **Gauss distribution** (Fig. 4). In theory, it is for continuous numbers (those that can have decimal places) without lower or upper limits, but it can be, and is used to model all kinds of data sets that have a bell-shaped, symmetrical distribution.

![Figure 4: The normal distribution](/assets/images/gaussian.png)

*Figure 4: The normal distribution*

The **normal distribution** has a number of very special attributes I will not further elaborate on here. And unexpectedly it does also have formula, this one is a bit more unwieldy though:

$$
P(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

The important point to take home from this formula (I do not think you need to learn it) is that it takes two parameters to describe the distribution: $\mu$ and $\sigma$. $\mu$ Stands for the arithmetic mean of the distribution, its most probably value, i.e. the x-position of the peak of the curve. $\sigma$ Is the standard deviation. You may wonder why mean and standard deviation are called $\mu$ and $\sigma$ here, whereas I used 
$\bar{x}$ and $s$ in chapter 2? Well, according to convention, $\bar{x}$ is for empirical estimates calculated on samples of real observations, $\mu$ is for mathematical models and populations. The same applies to standard deviation.

## The t-value

** How do I compare two means?**

Now, back to hypothesis testing. Imagine the following research question: 

> **Were swords during the Viking age longer than in the migration period?**

I've been repeatedly criticized for overusing biological examples, like human body height. So I try coming up with something humanities-related. The numbers I use as an example here are pure inventions though.

We want to answer this question based on two sets of measurements. We have measured 10 Viking age weapons

> 88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0

and 10 *Spatha* type swords from the migration period.

> 72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6

To compare these two series of numbers, we begin by appling descriptive statistics to summarized them:

> Viking age
> 
> sample size: 10
> 
> mean: 89.95 cm
> 
> standard deviation: 2.6 cm
> 
> 
> migration period
> 
> sample size: 10
> 
> mean: 77.9 cm
> 
> standard deviation: 5.4 cm

One obvious way to describe the difference would be to report the difference of means:

> The Viking age swords in this sample are at average 12 cm longer than the migration period weapons.

However, we have to account for the standard deviation. An average difference of 12 cm is simply less meaningful if the standard deviations are very high and the the distributions overlap a lot. This is where the ***t*** **value** comes to play. The *t* value standardizes the difference of means to the standard deviation. It is much less interpretable than the difference of means, but much more meaningful. It is calculated as:

$$
t = \frac{\bar{X}_1 - \bar{X}_2}{s_p \sqrt\frac{2}{n}}
$$
with
$$
s_p = \sqrt{\frac{s_{X_1}^2+s_{X_2}^2}{2}}.
$$

In our example the *t* value is 6.319. But what does that mean? To this point, we just established a new descriptive statistic that allows us to report standardized differences of means. We can quantify the difference now, but how can we construct a null hypothesis to approach our research question?

Image there is no real difference. The one we observe is simply a result of random chance. We took two samples of ten swords, randomly, and by accident more of the longer blades ended up in the first sample. That happens in random processes, no need to assume a systematic influence. But how likely would our observation be under the assumption of randomness?

To answer that question we must come up with a meaningful **null hypothesis** that blames all the observed difference on random chance. The trick here is to assume that random sword lengths can be **modeled as a normal distribution**. Imagine the normal distribution is the random process that generated the numbers we find in both sets. Imagine it is a single normal distribution with one mean and one standard deviation. You can actually easily simulate that random process in your favorite programming language with a simple loop and a generator for random numbers from a normal distribution. Imagine we sample a list of 10 random numbers from that distribution. And then we draw a second sample of 10 numbers. We can calculate the *t* value for these two samples, and it will represent a difference caused by pure chance. Doing that only once is not too interesting, but computing many random *t* values can give you an impression of what to expect if the null hypothesis was true. If we compute a really large number random *t* values, we can see that they have a characteristic distribution (Fig. 5).

![Figure 5: The *t* distribution](/assets/images/t.png)

*Figure 5: The t distribution*

This distribution does not look much different than the *normal distribution*. However, there is a different mathematical model behind them which is why we call it a *t distribution* in this case. And with this model random probabilities for different *t* values can be calculated which means: *p* values can be calculated.

## Caveats

...

## The one-sample t-test

**How can I compare a single mean with an expected value?**

...

## Once again, the one-sample t-test

**What if samples are dependant?**

...

## How to do the t-test

### in Python
we can use a function from the `scipy` libarary to make the binomial test.
```
from scipy import stats
```

scipy.stats.ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, alternative='two-sided', trim=0)

There is a 
```

```


```

```

### in R
we can create two vectors with the example data.
```
viking = c(88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0)
migration = c(72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6)
```


To compare the two vectors, we simply type
```
t.test(viking, migration)
```


```
	Welch Two Sample t-test

data:  viking and migration
t = 6.319, df = 12.999, p-value = 2.664e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  7.930237 16.169763
sample estimates:
mean of x mean of y 
    89.95     77.90 
```


one sample t-test
```

```
