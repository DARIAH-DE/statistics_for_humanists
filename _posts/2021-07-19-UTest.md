---
title: 7. Comparing ranks with the U-test
tags: rank_sums, non-parametric_tests, Mann-Whitney-U-Test, Wilcoxon_Test
article_header:
  type: cover
  image:
---

By now we have seen null hypothesis-based techniques for categorical data (the binomial test) and for continuous variables belonging to the interval scale (*t*-test and *F*-test). It is time to talk about the ordinal scale, about procedures that deal with ranks.

We best start with an example. *Lysistrata* by Aristophanes is one of the most famous comedies from ancient Greece. It is still read, staged, and even re-interpreted by film makers, not least because it tells a story about women meddling in the back then very masculine domain of politics. One could argue that this play tells the story from the women's point of view, and one could derive the expectation that

> female characters appear on stage before male characters.

This is the example hypothesis we want to work on for the moment. The characters, in order of appearance, are

> 1. Lysistrata, 2. Kleonike, 3. Myrrhine, 4. Lampito, 5. Ismena, 6. a Skythian girl, 7. the men's chorus, 8. the women's chorus, 9. the magistrate, 10.-13. four Skythian guards, 14.-16. three old women, 17.-20. four women, 21. Kinesias, 22. a baby boy, 23. Manes, 24. a Spartan herald, 25. the Spartan ambassador, 26. the Athenian ambassador, 27.-28. two Athenian delegates, 29.-30. two Spartan delegates, 31. the Goddess Harmonia

I took the liberty to count the choruses as single characters. If we replace character name by gender, the list looks like this:

> 1. female, 2. female, 3. female, 4. female, 5. female, 6. female, 7. male, 8. female, 9. male, 10.-13. male, 14.-16. female, 17.-20. female, 21. male, 22. male, 23. female, 24. male, 25. male, 26. male, 27.-28. male, 29.-30. male, 31. female

How can we construct a null hypothesis for this case and tests it?

## Rank sums and *U*-values

Let us look at the data in terms of two series of ranks. The *ranks of appearance* of the **female characters** are:

> 1, 2, 3, 4, 5, 6, 8, 14, 15, 16, 17, 18, 19, 20, 21, 23, 31

For the **male characters**, we have:

> 7, 9, 10, 11, 12, 13, 21, 22, 24, 25, 26, 27, 28, 29, 30

Hypothetically, we could calculate mean ranks for both groups. We could even squeeze the numbers into the *t* test algorithm and receive no complains from it. Conceptually however, that would make little sense. There is no good reason to believe ranks to be normally distributed, therefore *t*-values are hardly a good representation for the data. 

The procedure we will use instead is known as the **Mann-Whitney-U test**, **Wilcoxon rank-sum test**, **Wilcoxon-Mann-Whitney test**, **U test** or **rank sum test**. It is modeled after the following **null hypothesis**:

> $H_0:$ The probability, that a randomly chosen rank from sample A is larger than a randomly chosen rank from sample B is 0.5.

Or in short:

> $H_0: P(A>B) = P(B>A) = 0.5$

As one of its names suggests quite obviously, the test is based on calculating **rank sums**. In our example, if we sum up both lists of ranks, we get ***223*** for the female characters and ***294*** for the male characters. Based on these rank sums a measure named **U** - you already guessed it - is calculated. It is calculated as

$$
U = n_1 n_2 + \frac{n_1(n_1+1)}{2}-R_1
$$

with $R_1$ being the larger of the two rank sums, $n_1$ the sample size of the series with the larger rank sum, $n_2$ the size of the sample with the smaller rank sum.

As with the *t* and the *F* value, the *U* statistic too follows a predictable distribution under random conditions. Much like in the procedures introduced before, we can associate an observed *U* value with a probability, i.e. a **p-value** if assuming the null hypothesis to be true. In this example, the *p*-value is *0.04*. Therefore, at a significance level of $\alpha = 0.05$, we can say that women appear in the play significantly earlier than male characters.

## Non-parametric tests

In contrast to the *t*-test, the *U*-test is what statisticians call a **non-parametric test**. Unlike **parametric tests** it does not make assumptions about distributions and corresponding model parameters that ought to have produced the observed data assuming the null hypothesis was true. Non-parametric tests rely on fewer assumptions than parametric tests. They can be applied to a wider range of cases as they have fewer prerequisites. 

## A safer alternative to the *t*-test?

Many users of statistical software packages like to see the *U* as a *safer alternative* to the *t* test that can be applied to the same scenarios. And it is certainly true that any sample of numbers used in a *t* test can easily be turned into ranks to apply a *U* test instead. It is also true that when doing so, you do not need to fear about violating distributional assumptions of the underlying model, because in the *U* test there are none. So why should we use *t* tests at all, aren't we on the safe side if relying exclusively on non-parametric procedures?

There are arguments against that strategy. First, the two procedures are modeling **different null hypotheses**. One looks at differences of means while the other asks how likely a number from one set is larger than a number from the other. I know, for the average researcher just keen to know if two samples are anyhow different, these conceptual subtitles often make little difference.

Therefore, you should consider the second argument: if model assumptions are close to correct, the *t* test has more **statistical power** meaning it can detect an existing effect with less observations. I deliberately say *close to correct* because *t* tests have been shown to work quite robustly if assumptions are violated to a moderate degree.

And if you think your data violates the assumption of normal distribution more than moderately, I would always advise to **consider a transformation** before switching to the non-parametric alternative. *log* Transformations often work on exponential data, the *arcsin* transformation can at times bring proportions or percentages close to a normal distribution. I have heard people whining "But I transform the numbers, they are not my measurements any more. Can I really just change the numbers because the *t* test doesn't like them? That feels like cheating!" It is not. You treat every number equally. And guess what happens if you throw your preciously precise measurements into a *U* test function? It will *transform* them into ranks! $-5.5$, $7$, $9.81$ and $6.022*10^{23}$ will be turned into $1$, $2$, $3$ and $4$. If you consider that a more truthful representation of your measurements, go ahead. This illustrates the third argument against an indiscriminate use of non-parametric tests for numerical data: they **use less information**, rely on a more abstract, less precise model of your observational data.

The *U* test is made for rank data. It is fine to use it on real numbers too, if parametric tests cannot be applied in good conscience, but other options should be considered beforehand.


## How to do the U-test

### in Python

Once again, we will need `numpy` and `scipy` for the implementation.
```
import numpy as np
from scipy import stats
```


The data from the example used above:
```
female = np.array([1, 2, 3, 4, 5, 6, 8, 14, 15, 16, 17, 
	18, 19, 20, 21, 23, 31])

male = np.array([7, 9, 10, 11, 12, 13, 21, 22, 24, 25, 
	26, 27, 28, 29, 30])
```


The function performing *U* test is named after Frank Wilcoxon in this case:
```
stats.mannwhitneyu(female, male)
```


```
MannwhitneyuResult(statistic=72.5, pvalue=0.019782673982575185)
```

### in R

The data from the example used above:
```
female = c(1, 2, 3, 4, 5, 6, 8, 14, 15, 16, 17, 
	18, 19, 20, 21, 23, 31)

male = c(7, 9, 10, 11, 12, 13, 21, 22, 24, 25, 
	26, 27, 28, 29, 30)
```


The function performing *U* test is named after Frank Wilcoxon in this case:
```
wilcox.test(female, male)
```


```
Wilcoxon rank sum test with continuity correction

data:  female and male
W = 72.5, p-value = 0.03957
alternative hypothesis: true location shift is not equal to 0
```