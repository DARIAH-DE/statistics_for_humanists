---
title: 6. Comparing two variances with the F-test
tags: homogeneity_of_variance
article_header:
  type: cover
  image:
---


After we introduced a number of new concepts in the last two chapters, this one will be quite short. It presents another important significance test based on the concepts we already know now.

## The F-test

**How do I compare two variances?**

In the last chapter we used some imagined data about sword lengths in samples from the Viking age 

> 88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0

and the migration period.

> 72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6

We asked if the means of these two samples were significantly different, and the answer was no. Now we want to ask, if their variances differ significantly.

The Viking age weapons have a variance of 6.905, for the migration period swords it is 29.46. Like in the *t*-test, this difference can be analyzed with two concepts:

1. A statistic to quantify the difference of variabilities. 

2. A probabilistic model of what that statistic should look like assuming it is a result of random processes.

The statistic used in this case is quite easy, it is the *F* value, i.e. the variance of one sample divided by the variance of the other.

$$
F = \frac{s_1^2}{s_2^2}
$$

The rest is pretty much straightforward if we understood how the *t*-test works: if we take two samples from the **same** normal distribution, calculate the *F* value for the two samples, and repeat that process many times, we will see that random *F* values follow a certain distribution model. This model is known as - you can guess it - the ***F***-**distribution**, or **Fisherâ€“Snedecor distribution** after the people who first described it, and its analytical solution allows us to estimate the probability of an observed *F* value under the assumption that both samples come from normal distributions with equal variances, and that their variances are different only due to the probabilistic effects of sampling. 

This whole procedure is called - you guess it once again - the ***F***-**test**. Its most important assumption is that is analyzes samples from normally distributed populations. Its formal null hypothesis is:

> $H_0:$ Both samples are from populations of the same variance.

or, shorter:

$$
H_0: \sigma_1^2 = \sigma_2^2
$$

The *F*-test is quite important beyond its "isolated" application as shown here. It is an integral component of the significance testing procedures applied to *linear regression* analysis and related methods like *ANOVA* and *ANCOVA*.

## How to do the F-test

### in Python
Again, we will use `numpy` to vectorize our data and `scipy` to provide another function.
```
import numpy as np
from scipy import stats
```


First, we create two vectors with the example data. 
```
viking = np.array([88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0])
migration = np.array([72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6])
```


Unfortunately, `scipy` offers no implementation of the *F*-test we can use directly for such a simple case. We have to implement our own little function to do that. Calculating the *F* value itself is quite straightforward. Turning it into *p*-value is done with a function from `scipy` that needs to know the *degrees of freedom* (sample size - 1) for each sample.
```
def f_test(x, y):
    F = np.var(x) / np.var(y)
    df_x = len(x)-1 #degrees of freedom x
    df_y = len(y)-1 #degrees of freedom y 
    p = 2*stats.f.cdf(F, df_x, df_y) #get two-tailed p-value
    return F, p
```


Now, we can apply the test:
```
f_test(viking, migration)
```


```
(0.23438560760353006, 0.04176389929283543)
```

What does the result tell us here? *F* is 0.234 with a *p*-value of 4.2%. At a 5% significance level we can say that the two samples have significantly different variances.

### in R
we will use the same two vectors as in the last chapter.
```
viking = c(88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0)
migration = c(72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6)
```


Doing the *F*-test is as straightforward as the other tests:
```
var.test(viking, migration)
```


```
	F test to compare two variances

data:  viking and migration
F = 0.23439, num df = 9, denom df = 9, p-value = 0.04176
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
 0.05821807 0.94363509
sample estimates:
ratio of variances 
         0.2343856 

```


What does the result tell us here? *F* is 0.234 with a *p*-value of 4.2%. At a 5% significance level we can say that the two samples have significantly different variances.
