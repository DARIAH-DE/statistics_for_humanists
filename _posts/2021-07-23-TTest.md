---
title: 5. Comparing two means with the t-test
tags: distributions, t-test, population_and_sample, dependant_samples, paired_test, sample_and_population
article_header:
  type: cover
  image:
---

The binomial test I introduced in the previous chapter has a great didactic advantage (which is why I introduced it first): it allows to calculate *p* values directly from the observed numbers. Tests that work like this are called *exact tests*. Unfortunately, not every test scenario makes it that easy for us. To be honest, most test scenarios don't. While plain count data is comparatively easy to treat with probabilistic concepts, many measurements we encounter in the real world of research produce more complex sets of numbers (often even with decimals and the like). To construct a null hypothesis and estimate probabilities for such measurements, an additional layer of abstraction is needed; instead of the *exact* numbers themselves, analysis has to be made on derived statistics. To understand how that works, we have to familiarize with another important concept: the *statistical distributions*

## Statistical distributions

**How to model data created by random chance?**

A meaningful null hypothesis assumes that observed data has been created by random processes. A statistical test provides us with an estimate for the probability of our observation under these random conditions. Accordingly, a statistical test is based on model of random data generation for a specific scenario. Mathematicians have described a large number of probabilistic processes that produce random numbers in different types and shapes. These are called *statistical distributions*. 

The simplest statistical distribution is the **uniform distribution**. Here, the spectrum of potential observed values has a lower and upper limit, and all values are equally likely. For an example histogram of random numbers sampled from a uniform distribution see Fig.1.

![Figure 1: The uniform distribution](uniform.png)

*Figure 1: Sample from a uniform distribution*

Now, let us assume a more complex random process. Imagine an archaeological excavation. The entire area under study is divided in equally sized squares, and our task is to count the occurrences of a specific type of archaeological object, let us say arrow heads, in each of the squares. In this case, a random process resulting in a uniform distribution seems not very plausible. The type of model we are looking for has two important features:

1. We have have a count of occurrences as a natural number for each discrete unit.

2. There is no natural upper limit to the number of occurrences in in each unit. The lower limit however is 0!

This type of probabilistic process can be modeled with the **Poisson distribution** (Fig. 2). This is not a weird reference to some attribute of aquatic vertebrates, it was just invented by a mathematician named SimÃ©on Denis Poisson.

![Figure 2: The Poisson distribution](/assets/images/poisson.png)

*Figure 2: Sample from a Poisson distribution*

According to his model, the probability to find $k$ arrow heads in a square is:

$$
P_\lambda (k) = \frac{\lambda^k}{k!}\, \mathrm{e}^{-\lambda}
$$

Here, $\lambda$ is the parameter describing the center of the distribution: its most probable value. It is the only parameter in the formula.

In many cases, observed values do have an upper limit. For instance, we often measure proportion or percentages in research that have an upper limit at 100%. These cases can be modeled with the **binomial distribution** (Fig. 3), and we already did that, though unconsciously, for the binomial test in Chapter 3. 

![Figure 3: The binomial distribution](/assets/images/binomial.png)

*Figure 3: The binomial distribution*

The mathematical expression for the probability that a sample of $n$ observations contains $k$ *successes* (or *heads*, or other binary events), given a single trial success probability of $p$ is:

$$
P_k = \binom nk p^k (1-p)^{n-k}
$$

Yes, we already know this formula from the previous chapter.

The most famous among the statistical distributions is the **normal distribution** AKA **Gauss distribution** (Fig. 4). In theory, it is for continuous numbers (those that can have decimal places) without lower or upper limits, but it can be, and is used to model all kinds of data sets that have a bell-shaped, symmetrical distribution.

![Figure 4: The normal distribution](/assets/images/gaussian.png)

*Figure 4: Sample from a normal distribution*

The **normal distribution** has a number of very special attributes I will not further elaborate on here. And not unexpectedly it does also have a formula, this one is a bit more unwieldy though:

$$
P(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

The important point to take home from this formula (I do not think you need to learn it) is that it takes two parameters to describe the distribution: $\mu$ and $\sigma$. $\mu$ Stands for the arithmetic mean of the distribution, its most probable value, i.e. the x-position of the peak of the curve. $\sigma$ Is the standard deviation. You may wonder why mean and standard deviation are called $\mu$ and $\sigma$ here, whereas I used 
$\bar{x}$ and $s$ in chapter 2? Well, according to convention, $\bar{x}$ is for empirical estimates calculated on samples of real observations, $\mu$ is for mathematical models and populations. The same applies to standard deviation.

An interesting point you can observe when experimenting with random samples from these distribution models in your faviourite scripting envisonment: if you draw large samples from *Poisson* or *normal* distributions and the sample values far away from the limits (i.e. values like 0, 0% or 100%), the samples become increasingly more similar to the normal distribution.

## The t-value

**How do I compare two means?**

Now, back to hypothesis testing. Imagine the following research question: 

> **Were swords during the Viking age longer than in the migration period?**

I've been repeatedly criticized for overusing biological examples, like human body height. So I try coming up with something humanities-related. The numbers I use as an example here are pure inventions though.

We want to answer this question based on two sets of measurements. We have measured 10 Viking age weapons

> 88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0

and 10 *Spatha* type swords from the migration period.

> 72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6

To compare these two series of numbers, we begin by appling descriptive statistics to summarize them:

> Viking age:
> 
> sample size: 10
> 
> mean: 89.95 cm
> 
> standard deviation: 2.6 cm
> 
> 
> migration period:
> 
> sample size: 10
> 
> mean: 77.9 cm
> 
> standard deviation: 5.4 cm

One obvious way to describe the difference would be to report the difference of means:

> The Viking age swords in this sample are at average 12 cm longer than the migration period weapons.

However, we have to account for the variability. An average difference of 12 cm is simply less meaningful if the standard deviations are very high and the the distributions overlap a lot. This is where the ***t*** **value** comes to play. The *t* value standardizes the difference of means to the standard deviation. It is much less interpretable than the difference of means, but much more meaningful. It is calculated as:

$$
t = \frac{\bar{X}_1 - \bar{X}_2}{s_p \sqrt\frac{2}{n}}
$$
with
$$
s_p = \sqrt{\frac{s_{X_1}^2+s_{X_2}^2}{2}}.
$$

## From t to p

In our example the *t* value is 6.319. But what does that mean? To this point, we just established a new descriptive statistic that allows us to report standardized differences of means. We can quantify the difference now, but how can we construct a null hypothesis to approach our research question?

Well, image there was no real difference. The one we observe is simply a result of random chance. We took two samples of ten swords, randomly, and by accident more of the longer blades ended up in the first sample. That happens in random processes, no need to assume a systematic influence. But how likely would our observation be under the assumption of randomness?

To answer that question we must come up with a meaningful **null hypothesis** that blames all the observed difference on random chance. The trick here is to assume that random variability in sword length can be **modeled as a normal distribution**. Imagine the normal distribution to be the random process that generated the numbers we find in both sets. Imagine it was a single normal distribution with one mean and one standard deviation. You can actually easily simulate that random process in your favorite programming language with a simple loop and a generator for random numbers from a normal distribution. We can sample a list of 10 random numbers from that distribution. And then we draw a second sample of 10 numbers. We can calculate the *t* value for these two samples, and it will represent a difference caused by pure chance. Doing that only once is not too interesting, but computing many random *t* values can give us an impression of what to expect if the null hypothesis was true. If we compute a really large number random *t* values, we can see that they have a characteristic distribution (Fig. 5).

![Figure 5: The *t* distribution](/assets/images/t.png)

*Figure 5: A t distribution*

This distribution does not look much different than the *normal distribution*. However, there is a different mathematical model behind is which is why we call it another name. And this model of random probabilities for different *t* values can be described as a formula and solved analytically, which means: *p* values can be calculated. Again, if *p* is smaller than $\alpha$ (the significance level, usually 5%), we assume our observation to be so unlikely under random conditions that the null hypothesis probably is not true. What is the **null hypothesis** in this test scenario?

> $H_0:$ Both sets of numbers are random samples from the same normal distribution.

This procedure is known as the *t*-test.

## Caveats

The *t*-test is a very popular method. However, it comes with some caveats!

**Very small sample size**

Actually, 10 measurements per sample is very very much on the low end of what is acceptable as a sample size for the *t*-test. 
As a student I was taught to repeat an experiment at least 15 times to achieve meaningful *t*-test results. 20 Is better than 15, 30 is better than 20. But...


**Very large sample size**

if we work with digitally generated data we can come across samples of 1000, 10000, or 100000 observations. These do not make a *t*-test less accurate, on the contrary, but it can become so sensitive to the smallest differences that a distance-of-means so small it hardly has any meaning in the real world is indicated as statistically significant. Accordingly, there are two points to bear in mind:

1. The *t*-test is popular because it worked well for many researchers in the past in a medium range of sample sizes.

2. **Statistical significance** and real world **relevance** are two very **different** concepts!

**The assumption of normality**

The *t*-test assumes that the observed data is generated by a probabilistic process that can be modeled by the *normal distribution*. The more the reality of your data is in violation of that assumption, the less accurate the result of the test, it is generally not a good idea to use a *t*-test if your data is heavily skewed or otherwise producing exotic shapes when plotted in a histogram. There are ways to help you guessing if your data may fit to a normal distribution, for example

1. there may be inherent ***mathematical reasons*** to believe so.
2. Or you can try a visual diagnosis; a ***histrogram*** may do the job, but a ***normal Q-Q plot*** is usually better to discover deviations from Gaussian normality.
3. And there are even significance tests that test *for normality*, like the **Shapiro-Wilk test** or the **Komolgorov-Smirnov test**.

As significance tests for normal distributions are not too reliable on smaller sample sizes it is always advisable to look at visualizations too. And you should always keep in mind: It is actually not your sample, but the population it has been drawn from that the method assumes to be normally distributed.

Skewed samples can often be pushed towards a normal distribution by **transformation**. For instance in many cases working with the logarithms of all the measurement values can work wonders.

**The assumption of equal variances**

Another assumption of the *t*-test is that both samples have roughly the same variance. If in doubt, there is a another significance test to check for equal variances that can be done before the *t*-test. It is known as the ***F***-**test** and I will introduce it in the next chapter.

If the assumption of equal variances is violated, this is not a big problem, because there is a variant of the *t*-test called **Welch-test** that treats the sample variances a little differently and works reliably without equal variances.

## Population and sample

Then, there is a more fundamental caveat we have to keep in mind for every statistical test described in this entire tutorial. Each and every one of these procedures assumes that you want to analyze a sample. A sample representative of an unknown population. As we saw further above, mathematicians consider that difference so important that they even use different letters for sample and population mean, $\bar(x)$ and $\mu$ respectively. In academic reality we find a spectrum of degrees to which this assumption can be considered fulfilled. Doing measurements on a sample of people to gain knowledge about the human species in general fits to that concept more or less (you may often have discussions about the validity of the sampling process though). In computational text analysis, it is often more complicated. Computer linguists sometimes argue their corpus to be a sample representative for the general use of the language. But in computational literary studies, if you do research, for example, on Shakespeare you usually have about all the texts Shakespeare wrote to "measure" and analyze. Is your text collection the population of interest? Then you may be violating an assumption of working on a sample if you use a statistical test. Do you consider your collection as a sample? Then what is the population? Shakespeare's *literary language*, all the texts he could have written in theory? 

In some cases, there may be no answer you are completely comfortable with as a researcher. That is not a general argument against the use of statistical analysis, but a reminder to keep in mind that they are based on models of reality, that models have assumptions that are often unrealistic (and yes, that happens even in physics), and that **significance tests are no truth-machines** but mere examination aids to make a bit more sense of numbers. There is that famous aphorism, usually attributed to George Box, that you should always keep in mind:

> All models are wrong, but some of them are useful.


## The one-sample t-test

**How can I compare a single mean with an expected value?**

And there are more variants in this family of statistical procedures. The *t*-test can also be used to compare a sample with an expected mean value. In such a *one sample t-test*, the null hypothesis is:

> $H_0:$ The observed set of numbers is a random sample from a normal distribution with the specified mean value.

## Dependent samples

In research, there are often cases where we want to compare two sets of numbers that are dependent. In statistics, we consider two sets of measurements dependent if each value in one set has a direct, influencing relation to one value in the other set. If I want to compare body height between 5th graders and 6th graders, I can do that by measuring two entirely different samples of students, or I can measure the same group of kids twice with a year in between. In the latter case, each student is represented by a measurement in both of the sets, therefore the sets are dependent. Other very typical examples are measurements made on patients before and after a treatment.

This type experimental design requires a different statistical approach in analysis, usually called a **paired** test design, but for *t*-test-like situations it is quite easy to implement. Software packages and libraries usually offer a **paired t-test** for these cases. What such a paired *t*-test does is subtracting individually - in our example converting two sets of ages into one set of age differences - and making a one sample *t*-test with the vector of differences against an expected mean of 0.  

## How to do the t-test

### in Python
We will need `numpy` to vectorize our data and functions, and `scipy` for the test.
```
import numpy as np
from scipy import stats
```


First, we create two vectors with the example data. 
```
viking = np.array([88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0])
migration = np.array([72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6])
```


To **compare the two vectors**, we simply type
```
stats.ttest_ind(viking, migration)
```


```
Ttest_indResult(statistic=6.318954830932814, pvalue=5.903595140302869e-06)
```


If we want to use the **Welch test** instead, there is an argument telling the function to use the version of the test not assuming equal variances.
```
stats.ttest_ind(viking, migration, equal_var = False)
```


```
Ttest_indResult(statistic=6.318954830932814, pvalue=2.66401671214937e-05)
```


The **one sample t-test** is done with another function. Here, the expected mean has to be specified. For instance, many books state that a typical Viking age sword is about 90 cm long. Does our (invented) example data "confirm" that expectation?
```
stats.ttest_1samp(viking, 90) 
```


```
Ttest_1sampResult(statistic=-0.06017112935710659, pvalue=0.9533342891179046)
```


### in R
we can create two vectors with the example data.
```
viking = c(88.5, 84.9, 90.0, 90.1, 89.6, 95.2, 92.0, 88.9, 89.3, 91.0)
migration = c(72.5, 73.9, 83.1, 78.8, 89.2, 75.2, 79.8, 70.8, 79.1, 76.6)
```


To **compare the two vectors**, we simply type
```
t.test(viking, migration)
```


```
	Welch Two Sample t-test

data:  viking and migration
t = 6.319, df = 12.999, p-value = 2.664e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  7.930237 16.169763
sample estimates:
mean of x mean of y 
    89.95     77.90 
```

The output tells us that `R` on its own decided variances of the two sets to be too different, and chose to use the *Welch Two-Sample t-test*. If you want to suppress that kind of autonomy and get the *vanilla* version of the *t*-test, you can simply tell are in advance that variance are equal.
```
t.test(viking, migration, var.equal = TRUE)
```


```
	Two Sample t-test

data:  viking and migration
t = 6.319, df = 18, p-value = 5.904e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  8.043624 16.056376
sample estimates:
mean of x mean of y 
    89.95     77.90 
```


The **one sample t-test** can be done by specifying an expectation value. For instance, many books state that a typical Viking age sword is about 90 cm long. Does our (invented) example data "confirm" that expectation?
```
t.test(viking, mu = 90)
```


```
	One Sample t-test

data:  viking
t = -0.060171, df = 9, p-value = 0.9533
alternative hypothesis: true mean is not equal to 90
95 percent confidence interval:
 88.07023 91.82977
sample estimates:
mean of x 
    89.95 
```


Like the binomial test, the *t*-test can also be made as a **one-sided** test.
```
t.test(viking, migration, alternative = "greater")
```


```
	Welch Two Sample t-test

data:  viking and migration
t = 6.319, df = 12.999, p-value = 1.332e-05
alternative hypothesis: true difference in means is greater than 0
95 percent confidence interval:
 8.672884      Inf
sample estimates:
mean of x mean of y 
    89.95     77.90 
```
